Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
num_params = 85,800,194 | trainable_params = 1,538
C:\Users\win10\AppData\Local\Temp\ipykernel_20448\91542603.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Training and evaluating model: google/vit-base-patch16-224
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [11:01<00:00,  3.87s/it]
No files have been modified since last commit. Skipping to prevent empty commit.
{'eval_loss': 0.5385066270828247, 'eval_accuracy': 0.8042328042328042, 'eval_sensitivity': 0.0, 'eval_specificity': 1.0, 'eval_average_precision': 0.1638865321325322, 'eval_auc_roc': 0.41607396870554764, 'eval_runtime': 38.9738, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 0.616, 'epoch': 1.0}
{'loss': 0.5469, 'grad_norm': 3.6283717155456543, 'learning_rate': 4.152046783625731e-05, 'epoch': 1.75}
{'eval_loss': 0.5076493620872498, 'eval_accuracy': 0.8042328042328042, 'eval_sensitivity': 0.0, 'eval_specificity': 1.0, 'eval_average_precision': 0.19825895563639068, 'eval_auc_roc': 0.5197368421052632, 'eval_runtime': 36.6943, 'eval_samples_per_second': 5.151, 'eval_steps_per_second': 0.654, 'epoch': 2.0}
{'eval_loss': 0.501335859298706, 'eval_accuracy': 0.8042328042328042, 'eval_sensitivity': 0.0, 'eval_specificity': 1.0, 'eval_average_precision': 0.20813860626527453, 'eval_auc_roc': 0.5412517780938833, 'eval_runtime': 37.4841, 'eval_samples_per_second': 5.042, 'eval_steps_per_second': 0.64, 'epoch': 3.0}
{'train_runtime': 661.225, 'train_samples_per_second': 4.083, 'train_steps_per_second': 0.259, 'train_loss': 0.516750893397638, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:35<00:00,  1.48s/it]
