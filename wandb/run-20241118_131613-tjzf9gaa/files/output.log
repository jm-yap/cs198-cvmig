Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
num_params = 85,800,194 | trainable_params = 1,538
C:\Users\win10\AppData\Local\Temp\ipykernel_26168\844982615.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Training and evaluating model: google/vit-base-patch16-224
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [02:11<00:00,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:08<00:00,  2.67it/s]
{'eval_loss': 0.5834274888038635, 'eval_accuracy': 0.7301587301587301, 'eval_sensitivity': 0.4594594594594595, 'eval_specificity': 0.7960526315789473, 'eval_average_precision': 0.38624527612182574, 'eval_auc_roc': 0.7233285917496444, 'eval_runtime': 9.2918, 'eval_samples_per_second': 20.341, 'eval_steps_per_second': 2.583, 'epoch': 1.0}
{'loss': 0.6711, 'grad_norm': 2.815889596939087, 'learning_rate': 6.336996336996337e-05, 'epoch': 1.1}
{'eval_loss': 0.5479239821434021, 'eval_accuracy': 0.7724867724867724, 'eval_sensitivity': 0.4594594594594595, 'eval_specificity': 0.8486842105263158, 'eval_average_precision': 0.4435306813994576, 'eval_auc_roc': 0.7546230440967284, 'eval_runtime': 9.491, 'eval_samples_per_second': 19.914, 'eval_steps_per_second': 2.529, 'epoch': 2.0}
{'loss': 0.6093, 'grad_norm': 5.599250316619873, 'learning_rate': 2.673992673992674e-05, 'epoch': 2.2}
{'eval_loss': 0.5385267734527588, 'eval_accuracy': 0.7671957671957672, 'eval_sensitivity': 0.40540540540540543, 'eval_specificity': 0.8552631578947368, 'eval_average_precision': 0.4635930919147714, 'eval_auc_roc': 0.7633357041251778, 'eval_runtime': 12.4031, 'eval_samples_per_second': 15.238, 'eval_steps_per_second': 1.935, 'epoch': 3.0}
{'train_runtime': 131.1694, 'train_samples_per_second': 33.255, 'train_steps_per_second': 2.081, 'train_loss': 0.6268838316529661, 'epoch': 3.0}
